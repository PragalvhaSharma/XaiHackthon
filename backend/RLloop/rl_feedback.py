"""
RL Feedback Pipeline for Recruiter System

This module implements the reinforcement learning feedback loop:
1. Store AI scores
2. Collect recruiter feedback (1-5 stars)
3. Compute deltas
4. Update policy state with exponential decay weighting

The system learns from recruiter corrections to improve scoring accuracy over time.
"""

import sqlite3
from typing import Tuple, Optional
from pathlib import Path
import math

# Database path (adjust if needed)
DB_PATH = Path(__file__).parent.parent.parent / "data" / "recruiter.db"

# Star to score mapping (1-5 stars -> 0-100)
STAR_MAP = {
    5: 100,  # Exceptional fit
    4: 75,   # Strong fit
    3: 50,   # Good fit
    2: 25,   # Poor fit
    1: 0     # Not qualified
}

# RL hyperparameters
LEARNING_RATE = 0.1  # How quickly the model adapts to new feedback
DECAY_FACTOR = 0.95  # Exponential decay for older samples
MIN_WEIGHT = 0.01    # Minimum policy weight


def get_db_connection():
    """Create a database connection."""
    return sqlite3.connect(str(DB_PATH))


def update_ai_score(candidate_id: str, ai_score: int) -> None:
    """
    Store the AI-generated score in the candidates table.
    
    Args:
        candidate_id: Unique candidate identifier
        ai_score: Score from 0-100 generated by the LLM
    """
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute(
        "UPDATE candidates SET score = ? WHERE id = ?",
        (ai_score, candidate_id)
    )
    conn.commit()
    conn.close()


def compute_delta(ai_score: int, recruiter_stars: int) -> Tuple[int, int]:
    """
    Convert recruiter stars to 0-100 scale and compute delta.
    
    Args:
        ai_score: AI-generated score (0-100)
        recruiter_stars: Recruiter rating (1-5)
        
    Returns:
        Tuple of (recruiter_score, delta)
        - recruiter_score: Normalized 0-100 score
        - delta: recruiter_score - ai_score (positive = underrated, negative = overrated)
    """
    if recruiter_stars not in STAR_MAP:
        raise ValueError(f"Invalid star rating: {recruiter_stars}. Must be 1-5.")
    
    recruiter_score = STAR_MAP[recruiter_stars]
    delta = recruiter_score - ai_score
    return recruiter_score, delta


def store_reward(
    candidate_id: str, 
    job_id: str, 
    ai_score: int, 
    recruiter_score: int, 
    delta: int
) -> int:
    """
    Log the reward signal (delta) to the reward_log table.
    
    This creates a replay buffer for RL training.
    
    Args:
        candidate_id: Unique candidate identifier
        job_id: Job identifier
        ai_score: AI-generated score (0-100)
        recruiter_score: Recruiter score (0-100)
        delta: Difference between recruiter and AI scores
        
    Returns:
        The ID of the inserted reward log entry
    """
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO reward_log (
            candidate_id, 
            job_id, 
            ai_score, 
            recruiter_score, 
            delta, 
            created_at
        )
        VALUES (?, ?, ?, ?, ?, strftime('%s','now'))
    """, (candidate_id, job_id, ai_score, recruiter_score, delta))
    conn.commit()
    reward_id = cur.lastrowid
    conn.close()
    return reward_id


def update_policy_state(job_id: str, delta: int, version: int = 1) -> None:
    """
    Update the RL policy state using the new feedback delta.
    
    This implements a running average with exponential decay:
    - Recent samples have more weight
    - Model adapts quickly to new patterns
    - Weight decreases as error increases
    
    Args:
        job_id: Job identifier
        delta: Feedback delta (recruiter_score - ai_score)
        version: Policy version (default 1)
    """
    conn = get_db_connection()
    cur = conn.cursor()
    
    # Get current policy state
    cur.execute("""
        SELECT error_avg, sample_count, weight
        FROM policy_state
        WHERE job_id = ? AND version = ?
    """, (job_id, version))
    
    result = cur.fetchone()
    
    if result is None:
        # Initialize new policy state for this job
        error_avg = abs(delta)
        sample_count = 1
        weight = 1.0 / (1.0 + error_avg)
        
        cur.execute("""
            INSERT INTO policy_state (
                job_id, 
                version, 
                weight, 
                error_avg, 
                sample_count, 
                created_at
            )
            VALUES (?, ?, ?, ?, ?, strftime('%s','now'))
        """, (job_id, version, weight, error_avg, sample_count))
    else:
        # Update existing policy state with exponential decay
        old_error_avg, old_sample_count, old_weight = result
        
        # Apply exponential decay to old average
        decayed_avg = old_error_avg * DECAY_FACTOR
        
        # Compute new weighted average
        # new_error_avg = (1 - α) * decayed_old + α * new_error
        new_error = abs(delta)
        new_error_avg = (1 - LEARNING_RATE) * decayed_avg + LEARNING_RATE * new_error
        
        # Increment sample count
        new_sample_count = old_sample_count + 1
        
        # Compute new weight (inverse of error + 1)
        # Low error = high weight (trust the model)
        # High error = low weight (don't trust the model)
        new_weight = max(MIN_WEIGHT, 1.0 / (1.0 + new_error_avg))
        
        cur.execute("""
            UPDATE policy_state
            SET error_avg = ?,
                sample_count = ?,
                weight = ?
            WHERE job_id = ? AND version = ?
        """, (new_error_avg, new_sample_count, new_weight, job_id, version))
    
    conn.commit()
    conn.close()


def process_feedback(
    candidate_id: str, 
    job_id: str, 
    ai_score: int, 
    recruiter_stars: int,
    version: int = 1
) -> dict:
    """
    Complete RL feedback pipeline.
    
    This is the main entry point for processing recruiter feedback:
    1. Store AI score in candidates table
    2. Convert stars to normalized score
    3. Compute delta
    4. Log reward to reward_log table
    5. Update policy state with RL learning
    
    Args:
        candidate_id: Unique candidate identifier
        job_id: Job identifier
        ai_score: AI-generated score (0-100)
        recruiter_stars: Recruiter rating (1-5)
        version: Policy version (default 1)
        
    Returns:
        Dictionary with feedback results:
        {
            'candidate_id': str,
            'job_id': str,
            'ai_score': int,
            'recruiter_score': int,
            'delta': int,
            'reward_id': int
        }
    """
    # 1. Store AI score
    update_ai_score(candidate_id, ai_score)
    
    # 2. Compute delta
    recruiter_score, delta = compute_delta(ai_score, recruiter_stars)
    
    # 3. Log reward
    reward_id = store_reward(candidate_id, job_id, ai_score, recruiter_score, delta)
    
    # 4. Update RL policy state
    update_policy_state(job_id, delta, version)
    
    return {
        'candidate_id': candidate_id,
        'job_id': job_id,
        'ai_score': ai_score,
        'recruiter_score': recruiter_score,
        'delta': delta,
        'reward_id': reward_id
    }


def get_policy_stats(job_id: str, version: int = 1) -> Optional[dict]:
    """
    Get current RL policy statistics for a job.
    
    Args:
        job_id: Job identifier
        version: Policy version (default 1)
        
    Returns:
        Dictionary with policy stats or None if not found:
        {
            'job_id': str,
            'version': int,
            'weight': float,
            'error_avg': float,
            'sample_count': int,
            'created_at': int
        }
    """
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("""
        SELECT job_id, version, weight, error_avg, sample_count, created_at
        FROM policy_state
        WHERE job_id = ? AND version = ?
    """, (job_id, version))
    
    result = cur.fetchone()
    conn.close()
    
    if result is None:
        return None
    
    return {
        'job_id': result[0],
        'version': result[1],
        'weight': result[2],
        'error_avg': result[3],
        'sample_count': result[4],
        'created_at': result[5]
    }


def get_reward_history(job_id: str, limit: int = 50) -> list:
    """
    Get recent reward history for a job.
    
    Args:
        job_id: Job identifier
        limit: Maximum number of records to return
        
    Returns:
        List of reward log entries (most recent first)
    """
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("""
        SELECT 
            id,
            candidate_id,
            job_id,
            ai_score,
            recruiter_score,
            delta,
            created_at
        FROM reward_log
        WHERE job_id = ?
        ORDER BY created_at DESC
        LIMIT ?
    """, (job_id, limit))
    
    rows = cur.fetchall()
    conn.close()
    
    return [
        {
            'id': row[0],
            'candidate_id': row[1],
            'job_id': row[2],
            'ai_score': row[3],
            'recruiter_score': row[4],
            'delta': row[5],
            'created_at': row[6]
        }
        for row in rows
    ]


def compute_calibration_metrics(job_id: str) -> dict:
    """
    Compute model calibration metrics for a job.
    
    This analyzes how well-calibrated the AI scores are:
    - Mean absolute error (MAE)
    - Mean error (bias)
    - Root mean squared error (RMSE)
    
    Args:
        job_id: Job identifier
        
    Returns:
        Dictionary with calibration metrics
    """
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("""
        SELECT ai_score, recruiter_score, delta
        FROM reward_log
        WHERE job_id = ?
    """, (job_id,))
    
    rows = cur.fetchall()
    conn.close()
    
    if not rows:
        return {
            'job_id': job_id,
            'sample_count': 0,
            'mae': None,
            'bias': None,
            'rmse': None
        }
    
    deltas = [row[2] for row in rows]
    
    # Mean Absolute Error (average magnitude of error)
    mae = sum(abs(d) for d in deltas) / len(deltas)
    
    # Bias (systematic over/under estimation)
    # Positive = AI systematically underrates
    # Negative = AI systematically overrates
    bias = sum(deltas) / len(deltas)
    
    # Root Mean Squared Error (penalizes large errors)
    rmse = math.sqrt(sum(d**2 for d in deltas) / len(deltas))
    
    return {
        'job_id': job_id,
        'sample_count': len(deltas),
        'mae': round(mae, 2),
        'bias': round(bias, 2),
        'rmse': round(rmse, 2)
    }


if __name__ == "__main__":
    # Example usage
    print("RL Feedback Pipeline Example\n" + "="*50)
    
    # Simulate AI scoring a candidate
    candidate_id = "test_candidate_1"
    job_id = "swe-ai"
    ai_score = 82
    
    print(f"\n1. AI Score: {ai_score}/100")
    
    # Simulate recruiter feedback
    recruiter_stars = 3  # Good fit (50/100)
    print(f"2. Recruiter Stars: {recruiter_stars} ⭐")
    
    # Process feedback
    result = process_feedback(candidate_id, job_id, ai_score, recruiter_stars)
    
    print(f"\n3. Feedback Results:")
    print(f"   - Recruiter Score: {result['recruiter_score']}/100")
    print(f"   - Delta: {result['delta']} (AI {'overrated' if result['delta'] < 0 else 'underrated'} by {abs(result['delta'])} points)")
    print(f"   - Reward Log ID: {result['reward_id']}")
    
    # Get updated policy stats
    stats = get_policy_stats(job_id)
    if stats:
        print(f"\n4. Updated Policy State:")
        print(f"   - Weight: {stats['weight']:.4f}")
        print(f"   - Average Error: {stats['error_avg']:.2f}")
        print(f"   - Sample Count: {stats['sample_count']}")
    
    # Get calibration metrics
    metrics = compute_calibration_metrics(job_id)
    print(f"\n5. Calibration Metrics:")
    print(f"   - MAE: {metrics['mae']}")
    print(f"   - Bias: {metrics['bias']} (negative = overrating)")
    print(f"   - RMSE: {metrics['rmse']}")
